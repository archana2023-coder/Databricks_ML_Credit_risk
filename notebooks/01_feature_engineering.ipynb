# Databricks Notebook
# -------------------------------------
# Notebook: 02_feature_engineering
# Purpose : Create ML-ready features from credit card data
# Author  : Archana Ravuluri
# -------------------------------------
from pyspark.sql.functions import col, when
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler
DELTA_TABLE_PATH = "/FileStore/credit_data/delta/credit_transactions"

df = spark.read.format("delta").load(DELTA_TABLE_PATH)
display(df)
# select relevant columns
selected_cols = [
    "LIMIT_BAL",
    "AGE",
    "SEX",
    "EDUCATION",
    "MARRIAGE",
    "default_payment_next_month"
]

df = df.select(*selected_cols)
# handle missing values
df = (
    df
    .withColumn("AGE", when(col("AGE") <= 0, None).otherwise(col("AGE")))
    .dropna()
)
#Encode categorical features
sex_indexer = StringIndexer(inputCol="SEX", outputCol="SEX_indexed")
edu_indexer = StringIndexer(inputCol="EDUCATION", outputCol="EDUCATION_indexed")
mar_indexer = StringIndexer(inputCol="MARRIAGE", outputCol="MARRIAGE_indexed")

df = sex_indexer.fit(df).transform(df)
df = edu_indexer.fit(df).transform(df)
df = mar_indexer.fit(df).transform(df)

# Assemble feature vector
feature_cols = [
    "LIMIT_BAL",
    "AGE",
    "SEX_indexed",
    "EDUCATION_indexed",
    "MARRIAGE_indexed"
]

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="raw_features"
)

df = assembler.transform(df)
# Feature scaling
scaler = StandardScaler(
    inputCol="raw_features",
    outputCol="scaled_features"
)

scaler_model = scaler.fit(df)
df = scaler_model.transform(df)

#Final feature table
final_df = df.select(
    "scaled_features",
    col("default_payment_next_month").alias("label")
)

display(final_df)

# save features to delta
FEATURE_TABLE_PATH = "/FileStore/credit_data/delta/credit_features"

(
    final_df.write
    .format("delta")
    .mode("overwrite")
    .save(FEATURE_TABLE_PATH)
)
