# read features from features store
from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()

feature_df = fs.read_table("credit_default_features")
# Join with labels
labels_df = spark.table("silver_credit_card_transactions") \
    .select("customer_id", "default_payment_next_month") \
    .dropDuplicates()

dataset = feature_df.join(labels_df, "customer_id")
pdf = dataset.toPandas()
X = pdf.drop(columns=["customer_id", "default_payment_next_month"])
y = pdf["default_payment_next_month"]
# Train test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# Train model MLflow tracking
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

mlflow.set_experiment("/Shared/CreditCardDefaultPrediction")

with mlflow.start_run():

    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=8,
        random_state=42
    )

    model.fit(X_train, y_train)

    preds = model.predict(X_test)
    probs = model.predict_proba(X_test)[:, 1]

    mlflow.log_metric("accuracy", accuracy_score(y_test, preds))
    mlflow.log_metric("roc_auc", roc_auc_score(y_test, probs))

    mlflow.sklearn.log_model(
        model,
        artifact_path="credit_default_model",
        registered_model_name="CreditCardDefaultModel"
    )

