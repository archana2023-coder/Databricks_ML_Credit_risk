from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()
from pyspark.sql.functions import avg, sum, count
features_df = (
    spark.table("silver_credit_card_transactions")
    .groupBy("customer_id")
    .agg(
        avg("bill_amount").alias("avg_bill_amount"),
        sum("bill_amount").alias("total_bill_amount"),
        avg("payment_amount").alias("avg_payment_amount"),
        count("*").alias("txn_count")
    )
)
fs.create_table(
    name="credit_default_features",
    primary_keys=["customer_id"],
    df=features_df,
    description="Customer-level aggregated features for credit card default prediction"
)
training_df = fs.read_table("credit_default_features")
labels_df = spark.table("silver_credit_card_transactions") \
    .select("customer_id", "default_payment_next_month") \
    .dropDuplicates()

final_training_df = training_df.join(
    labels_df,
    on="customer_id",
    how="inner"
)


