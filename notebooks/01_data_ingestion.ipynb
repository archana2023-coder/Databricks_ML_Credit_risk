# Databricks Notebook
# -------------------------------------
# Notebook: 01_data_ingestion
# Purpose : Ingest raw credit card data into Delta Lake
# Author  : Archana Ravuluri
# -------------------------------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
RAW_DATA_PATH = "/FileStore/credit_data/raw"
DELTA_TABLE_PATH = "/FileStore/credit_data/delta/credit_transactions"
df_raw = (
    spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv(RAW_DATA_PATH)
)

display(df_raw)
# Row count check
print(f"Total records: {df_raw.count()}")

# Null check for critical columns
critical_cols = ["LIMIT_BAL", "AGE", "default_payment_next_month"]

for c in critical_cols:
    null_count = df_raw.filter(col(c).isNull()).count()
    print(f"{c} null count: {null_count}")
(
    df_raw.write
    .format("delta")
    .mode("overwrite")
    .save(DELTA_TABLE_PATH)
)
df_delta = spark.read.format("delta").load(DELTA_TABLE_PATH)
display(df_delta)
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS credit_transactions
    USING DELTA
    LOCATION '{DELTA_TABLE_PATH}'
""")
df_delta.select("AGE", "LIMIT_BAL", "default_payment_next_month").describe().show()

