# Load model from ML flow model registry
import mlflow.pyfunc

model_name = "CreditCardDefaultModel"
model_uri = f"models:/{model_name}/Production"

model = mlflow.pyfunc.load_model(model_uri)

#Prepare inference dataset( Feature store)
from databricks.feature_store import FeatureStoreClient

fs = FeatureStoreClient()

inference_df = fs.read_table("credit_default_features")

# convert into pandas
inference_pdf = inference_df.toPandas()
customer_ids = inference_pdf["customer_id"]
X_infer = inference_pdf.drop(columns=["customer_id"])
# run batch predictions
predictions = model.predict(X_infer)
import pandas as pd

pred_df = pd.DataFrame({
    "customer_id": customer_ids,
    "default_risk_score": predictions
})
# convert back to spark
pred_spark_df = spark.createDataFrame(pred_df)

#Store predictions in delta lake ( Gold layer)
pred_spark_df.write.format("delta") \
    .mode("overwrite") \
    .saveAsTable("gold_credit_default_predictions")




